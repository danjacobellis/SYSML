{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1fb27c97-222f-41f4-b247-bc263e9b1df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6145acd2-0216-4fc0-a2a9-a256b19747a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RGB_to_Y(x):\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    x = tf.reshape(x,(3,32,32))\n",
    "    x = tf.transpose(x,(1,2,0))\n",
    "    x = tf.image.rgb_to_grayscale(x)\n",
    "    return x\n",
    "def display_Y(Y):\n",
    "    return PIL.Image.fromarray(np.array(Y),'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "22e6edfd-615e-4832-a287-fa7f682c46a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = tf.map_fn(RGB_to_Y,val[\"data\"][0:5000])\n",
    "y = tf.convert_to_tensor(val[\"labels\"][0:5000])\n",
    "\n",
    "X_val = tf.map_fn(RGB_to_Y,val[\"data\"][-100:])\n",
    "y_val = tf.convert_to_tensor(val[\"labels\"][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "9fbb29de-9b76-4dcb-bd7f-68e5fdc67032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAC6ElEQVR4nAXBz2ucVRQG4Pecc78vM9NOk9DUJBWxVRoQpII/i1a6UqQrwZVCV936BwmiILpzL5S6sN2oFBR1U2xNaSxNSzBNJ+lMJvee9/V57LPFQKtHsw8GT3c3Osv9yb9b93yxzj9dz8OfXeWIITPDYGFrZfKc5Z3bJ2ZvPjjz0jnN3Qzu+9PaEmVIXxsPlHj7/OXNX0c/3oxBXxAR5WmvzmA9/MzMzHT3n8nLgzjPGxcCblCZFIyiRFjMDrCodu296VtaeLh9uHfKFTCfTibT+bxWv789eryvP0aX/h4/ubl88O4LiAKzcvQsfNA3s9FxrsfuL1fw2v3fVx5+PvImD6BUeGSN5LBwceve1fGzB/XF2xcdVtINsMsq/bCTPhy7pEh+/+rm2fO3/jr5/klNrmPFvlQhnX6yB1sgfWhH7Zgf3N65pCffnr5QijllbmbWBCYyTIr+DcwVSxtHpUmAvEwtdGfz8UEwRuNzr5T0ZHunuP2ZZDZFz+2vdmThclHl4sdhpNLKSIBBoH23a311E0qjbow+MQiUp5it1saWHvCMwXzeYPWnWueZUClGyYDYqbTBrKUcki9wfzXE9BIhsjOLbTT8V33e6Iw8vXT9ioRijpQCMMxOD1qKR9Os3nB26doPEQGWb8bHx8uLx473s1P7C6xQZ7UM1k5N9r6efhRi+U1JJRbWXrf0EWjWFOq74Y59cTerSks0svdnGVfJcJoSdHSDw9mjEbzQWRrNeqDQAik4zRTDQ+65oUzNo7W+UjQoJSMTALuZPSnWlY4BpNEDYDplRkDQic72ng+WKCI6c6e5w6ORgIQYpTU2L0STOlg/CIMECkiJWBIPD8wKpHQ2La+JIkCCKadWaSvNrcxBN7qyugiTtwRAT3BprESpVtFbKwKKKOfCjDA0tuF6QfEyNQsGauscpqSrrwK6lquteaKQ7pWmW496M2aTYI2gtr1ZU9oGrZToDBRE0syMzOIOZZP9D00E45k3sG3WAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACt0lEQVR4nAXBzY6eYxgH8P/1cT/P+2rN1BSTFjEJIcQsray67cIRcEa2Ns6AA+gZSIiInQidZCg2qtV5P57n/riuv99PPrsLkmTl8ccPrz56cv+Om5mpAKY2qmcRkOSvp9PHz9/68+SOm5mLiEmJjHS7hUyQ7336oDz+4u9zd3c1mJqmtNz66QlHQC4fKvDu+6ACJF08s9nM7ucbJo9nDxUgt/HPGyCRwiRnETW/y6w8+VwBfPv97/lmKNMsZs5UJYu3sa7lwRYAPpm/yWFFXc2lFCi1Qh1VlwsAwF/LGTlpmedp2rgWK7EtroK4vgWAzx5d1SLqJpOpTENhtooLsLl8vsXy5OubQ95WLzNEpsxt3zSZmqqI/HaFw09f/vzLq8ZQyck9J2UZxVQ9KVfy1eP9d/sPrq/PYVYkXCxLnwlJ0WDeu/fvox/uL//dvjBKZnFqEoUwnUw5bPvHJV47u6g1xc0Ugpw0VUq0VTXZ1pdeydfl7dFEIxTJKfoMGy2m7VG1935+c9Ei9wAgVGXd5qqHOXFzFIX1XLMcDusBvTIz0+NQGuUFqzR4rAiLpOSL006IcrRNHKfdNipyrV4xFAmSL6aVYZ6zHrZDZF/2ok6PtZUQCcRuaiJ99WXyZWoeY6q7snqtEROTuBGXYBRYT+9kNjoW+NLXsiiJpydNGEAzZ6h0z1znGr7sWhiEePpyZEIGGAUiskiP/WbxGtUKUvHsnUaR7BAfmxjSRbT18LqucyiB3dyVTCtJtsJEBe1gXuugqbDCBpEsw8c0OlIywkbzPnqmUnZlSBIIDu3zQITk6L56a81DKUdvQhF2qrJ6yAjqyPTeAyrCJl0syaoszIGQMRSreh+DosDA0IGEijS1IZGSrWR674OaQOQQkhpplKbBTMVSqrceVAUkByDIwbAEAGS3MfA/KGX02L90CRAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAADPklEQVR4nAXBS09cZRgA4Pf2nXPmBswALddisSoVYtQQknrZ2F/gzrhw5b8y7ow7F8bERJddaDRGkwY0EcqlIBQGBmbOzLl+3/v6PLgpSfvFqweP9+ESinbkCimhxdnSVozwHVIgZuZKSpKxkmu0I7HFSTm12mYmRB+UiIlrKzgE75kiD4gMATNEEjEQYmYGMvFZDLHPO+IQk6YM74gpokiIiYUdUBsZmxZ6LiZkh/UrJQykgYSJpfbcSQTYMXfAsLwpuS1EOh2rEBIJGuVoPbZif62jM5KjsSCEgSNCQuJoxY81DCC36jyvoKdZMAAEIhIEQK6oXwQ0/9pmfTSoC/PIagSAigIAxhHnrRTZJeudYZG5CzPsBwZzIci5b3WnDdNW4knvpXL/FNKZCXMP0QIGpHQCPLc2vTj/tIWLq0ujh3MxFGi2zmZWK8t8lsx0Oc3YPnhmCbmoyZ2AYBbQAvtIcu0sxbFdFJy8//KPp42rI+mxSy9/f0KmoCJau2w1iuZSH9WX13vNi7q3HR2cnd1NgKwxbIqxjWB6RkZV6DcnY233lt2Iz8Cqchxr18ntdH3ceHdKZ8vhPeT9+QU/7OuLG7DDU0yUSZrtenDW8fMr7Yl21u0Xj313OAS03NAVZISxyWgU/CibXKUYrUI/PzgeIaBFLQICychLNazKLLtRp75zbfpgkfcvccpbiCIiuz47eONtnOJUVz7thqZNhRC1P1xyG4nLSjAJKUn30VWelMlgY/L3x6vD3/z4dPlJqiNp1hAEAlB3++Q4nj26nMtZdpbivavr/x7u/BWXimyCIZa9+J3u80H+ydG/3Ggkt83tst4cnbTN1QoSxSJv/fDRg/r5Y7f2c/OWroa6NZtf7Do1UwySMG6svdm/Ll5feNk6L6t0A8zizB1HIUF1QQhoaa3Ll19/Md04vxv1f43UBo82DqvYu9KMCRB6EkJj8cf+5FsIXsd5OdiC8ziEAKhAADAFqrbwLPsmbVV1YIXPlwe7oIiqQUXJMj05P4irr8bQEQu123lPf3IySaRkDGLKfx4XQpD8c19MDSH+0u1ORBtRAK2UzOxKg3a2lwugGx8w+Wy2+f0QLTYjiv3/RxvN0M3XYFUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACoklEQVR4nD2Ty2pedRTFf2uf891ya0htmpqkDmIHdaBUSx0ITipephUnvoAP4DP4AI5FKg6cCFLBgRdwEHBgK6WiVKEIbdKmtA1q269fvsv57+XgpC72bMNiXfbWlxICROTGLsLC2AaDM0KHqFyNjtQRFQgkVMkmIlRFFRFofty1tFcFIahULFGHZAhHxjTdUx6ESaS0bBQhRb2rykH2lruONAKnsYG6AnsmKyi91cH+3xNsogABLnUYVJDLUuW5RZrN/RXjMGTaDkWE1nYjoqcymi2Mj98KhXAWgwgwHLuLOsOHl9//8/H642ndzUzTIsBJ7k/VaHLpzs5no3ploaSf7hWgUJz+kexc58ig88Wk25SZhGQ7HREhl+cfNPJPj9hbvXZnqbO7XDKbYimqKCXT+Pw2/WOLT848+uvn9asb07kKnJmlibRts753Zda9oR9ur3b/6C/GaBYCA0EaRZZ3t799iWfr3uTFJ1tNdz6aYsl2GMvF7nyw88zZm89d37j1yoTJQYBLMdY3UrZkit++z+GFtyb/TIszVWzs+rATjPP02a8ffH5qbRpZcAmSJJxpG9t2/+KZYedql3Tltm+o28ASLPbMm/OfnDxuHCUyLTLAGEAaXjq/snn0vYvXZCuMZEdgkAHzafdoOrde/fjfKtJqqcM4Wxuj319PZ/qNCx/dj0ppENRY7Y3r3qm1xO4sfxiX516TSAyRLm3a+evLB870tFp5++HJ727s3B0ChLO1aJ+bv4ntZrx/4p1fzm1tnlgA0Ff5VCPcawbLZD9euBLDgUinqfPQpYFVY0Q16UwH7e/ZdeK22HbA0u2l+4JWWl2k9P+FGXkydj9lTGLXqSoxsgU29tzGorZBrfSoVNKQltME9sFYs35mKSVLyf8Av5LKyQx+N6MAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAADXklEQVR4nAXByU5bVxgA4DP85547GeMBPGAwSWiA0qJGaqRIWVTqA2TRZ+iiz9VKfYBusu4m7arZRIpCKUOCwRh8r33nM/f78M8u6I+Cv6p8QX3PIwyAALgSpjEi/S9lAXRDl5a+estfvr8N4tBga4jnRj41bODvpn2CXH948fHmNVp2pqJJVs0j79D95XaTb/OqjEJyef7hOq6/VONqgg8a6n9WeLFn22r2FT5Lo1YXfpqz89kTdXY046xHElUU8sQkk9ke/UCHyDESDSzfng9VGuiW2MH1etWx2+kW5zY7rq/nOXmwfr/VhGEWy4Dk43WRHUvLRsNeeQAX/acTcjePwqZqloR4mnvhQk07Ru3cd8R1eP78+42APG/PE6ge7tWqJ9sIdWCsQnF4NU88ujhIL84I/7pVmFN+8im7P/RMa4AG6TS3s1IO0XT2X5RAkHcTydu8/66SwwHz3f7V3s4Ntj/G0j72JCNGKahQmW+9iKDtJ5uxhTV9lL3ZfC42uDciGOu7LrqdmQleXy3wEB6G4X29r/++eGBgaId4LhMi3fk2Z13jT4Za4oJ3mw13dMRvU+RRCHAJXrdbYIaxqfJz3U5mpqMdhoFABdsEX28Rp7LSiqxZAnJ80YXtNQkUKUOBUA1BtUxOaxrmqeCHdlXBJRS5N3uJ5+kxjUIMVWtcALv30edIPta1jLJVv91G78bFbjY/CTHp9r4jRUjyvNU5pBuKtzAGl7PR8PhUTwNDoZew6TLuigZntHeZxVzrhR4MBk9crZ9SJfFNZeQfcZi1ZD6vK42wG775VQUvukb8EEmo8FJasfzzrk1KL801ws7+Mnn87eZIDl6PYklKnEJl1vHlv7nd579ThNyzN5thsxIQEsaRLgFJZmPvcLcsm+zoE0HilVZN5XsIEWSRbIjQhEaErMpa0288hyZegGoAipG1ToqGVE47hxoeMWzosWueyQJZobQjxkhZ11BRhY3SmGDM3PgfvYN0YaxhAbJIiryCheWYNsZJpYQD2hNWKGSwolwJ05QCEhwCNc5WtTSOxExbrwRrPCxdpQotoCQyZI4KqY21yETaYWStc5KbRihRAwuZxdx64KGCgUIaN1YiTImQ0pi6htxRn/nYaoKllrWoKVWAELbWSG2k/h9mWAWphY5nVAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i_img in range(5):\n",
    "    img = display_Y(X[i_img,:,:,0])\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b227ffbe-8b2d-48d2-9896-b42ce63a6843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From Apache Licensed tensorflow compression example\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/data_compression.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_compression as tfc\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def make_analysis_transform(latent_dims):\n",
    "  \"\"\"Creates the analysis (encoder) transform.\"\"\"\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(\n",
    "          20, 5, use_bias=True, strides=2, padding=\"same\",\n",
    "          activation=\"leaky_relu\", name=\"conv_1\"),\n",
    "      tf.keras.layers.Conv2D(\n",
    "          50, 5, use_bias=True, strides=2, padding=\"same\",\n",
    "          activation=\"leaky_relu\", name=\"conv_2\"),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(\n",
    "          500, use_bias=True, activation=\"leaky_relu\", name=\"fc_1\"),\n",
    "      tf.keras.layers.Dense(\n",
    "          latent_dims, use_bias=True, activation=None, name=\"fc_2\"),\n",
    "  ], name=\"analysis_transform\")\n",
    "\n",
    "def make_synthesis_transform():\n",
    "  \"\"\"Creates the synthesis (decoder) transform.\"\"\"\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(\n",
    "          500, use_bias=True, activation=\"leaky_relu\", name=\"fc_1\"),\n",
    "      tf.keras.layers.Dense(\n",
    "          3200, use_bias=True, activation=\"leaky_relu\", name=\"fc_2\"),\n",
    "      tf.keras.layers.Reshape((8, 8, 50)),\n",
    "      tf.keras.layers.Conv2DTranspose(\n",
    "          20, 5, use_bias=True, strides=2, padding=\"same\",\n",
    "          activation=\"leaky_relu\", name=\"conv_1\"),\n",
    "      tf.keras.layers.Conv2DTranspose(\n",
    "          1, 5, use_bias=True, strides=2, padding=\"same\",\n",
    "          activation=\"leaky_relu\", name=\"conv_2\"),\n",
    "  ], name=\"synthesis_transform\")\n",
    "\n",
    "class Imgnet32yCompressionTrainer(tf.keras.Model):\n",
    "  \"\"\"Model that trains a compressor/decompressor for MNIST.\"\"\"\n",
    "\n",
    "  def __init__(self, latent_dims):\n",
    "    super().__init__()\n",
    "    self.analysis_transform = make_analysis_transform(latent_dims)\n",
    "    self.synthesis_transform = make_synthesis_transform()\n",
    "    self.prior_log_scales = tf.Variable(tf.zeros((latent_dims,)))\n",
    "\n",
    "  @property\n",
    "  def prior(self):\n",
    "    return tfc.NoisyLogistic(loc=0., scale=tf.exp(self.prior_log_scales))\n",
    "\n",
    "  def call(self, x, training):\n",
    "    \"\"\"Computes rate and distortion losses.\"\"\"\n",
    "    # Ensure inputs are floats in the range (0, 1).\n",
    "    x = tf.cast(x, self.compute_dtype) / 255.\n",
    "    x = tf.reshape(x, (-1, 32, 32, 1))\n",
    "\n",
    "    # Compute latent space representation y, perturb it and model its entropy,\n",
    "    # then compute the reconstructed pixel-level representation x_hat.\n",
    "    y = self.analysis_transform(x)\n",
    "    entropy_model = tfc.ContinuousBatchedEntropyModel(\n",
    "        self.prior, coding_rank=1, compression=False)\n",
    "    y_tilde, rate = entropy_model(y, training=training)\n",
    "    x_tilde = self.synthesis_transform(y_tilde)\n",
    "\n",
    "    # Average number of bits per MNIST digit.\n",
    "    rate = tf.reduce_mean(rate)\n",
    "\n",
    "    # Mean absolute difference across pixels.\n",
    "    distortion = tf.reduce_mean(abs(x - x_tilde))\n",
    "\n",
    "    return dict(rate=rate, distortion=distortion)\n",
    "\n",
    "def pass_through_loss(_, x):\n",
    "  # Since rate and distortion are unsupervised, the loss doesn't need a target.\n",
    "  return x\n",
    "\n",
    "def make_imgnet32y_compression_trainer(lmbda, latent_dims=50):\n",
    "  trainer =  Imgnet32yCompressionTrainer(latent_dims)\n",
    "  trainer.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    # Just pass through rate and distortion as losses/metrics.\n",
    "    loss=dict(rate=pass_through_loss, distortion=pass_through_loss),\n",
    "    metrics=dict(rate=pass_through_loss, distortion=pass_through_loss),\n",
    "    loss_weights=dict(rate=1., distortion=lmbda),\n",
    "  )\n",
    "  return trainer\n",
    "\n",
    "def add_rd_targets(image, label):\n",
    "  # Training is unsupervised, so labels aren't necessary here. However, we\n",
    "  # need to add \"dummy\" targets for rate and distortion.\n",
    "  return image, dict(rate=0., distortion=0.)\n",
    "\n",
    "def train_imgnet32y_model(lmbda):\n",
    "  trainer = make_imgnet32y_compression_trainer(lmbda)\n",
    "  trainer.fit(\n",
    "      training_dataset.map(add_rd_targets).batch(128).prefetch(8),\n",
    "      epochs=15,\n",
    "      validation_data=validation_dataset.map(add_rd_targets).batch(128).cache(),\n",
    "      validation_freq=1,\n",
    "      verbose=1,\n",
    "  )\n",
    "  return trainer\n",
    "\n",
    "class Imgnet32yCompressor(tf.keras.Model):\n",
    "  \"\"\"Compresses MNIST images to strings.\"\"\"\n",
    "\n",
    "  def __init__(self, analysis_transform, entropy_model):\n",
    "    super().__init__()\n",
    "    self.analysis_transform = analysis_transform\n",
    "    self.entropy_model = entropy_model\n",
    "\n",
    "  def call(self, x):\n",
    "    # Ensure inputs are floats in the range (0, 1).\n",
    "    x = tf.cast(x, self.compute_dtype) / 255.\n",
    "    y = self.analysis_transform(x)\n",
    "    # Also return the exact information content of each digit.\n",
    "    _, bits = self.entropy_model(y, training=False)\n",
    "    return self.entropy_model.compress(y), bits\n",
    "\n",
    "class Imgnet32yDecompressor(tf.keras.Model):\n",
    "  \"\"\"Decompresses MNIST images from strings.\"\"\"\n",
    "\n",
    "  def __init__(self, entropy_model, synthesis_transform):\n",
    "    super().__init__()\n",
    "    self.entropy_model = entropy_model\n",
    "    self.synthesis_transform = synthesis_transform\n",
    "\n",
    "  def call(self, string):\n",
    "    y_hat = self.entropy_model.decompress(string, ())\n",
    "    x_hat = self.synthesis_transform(y_hat)\n",
    "    # Scale and cast back to 8-bit integer.\n",
    "    return tf.saturate_cast(tf.round(x_hat * 255.), tf.uint8)\n",
    "\n",
    "def make_imgnet32y_codec(trainer, **kwargs):\n",
    "  # The entropy model must be created with `compression=True` and the same\n",
    "  # instance must be shared between compressor and decompressor.\n",
    "  entropy_model = tfc.ContinuousBatchedEntropyModel(\n",
    "      trainer.prior, coding_rank=1, compression=True, **kwargs)\n",
    "  compressor = Imgnet32yCompressor(trainer.analysis_transform, entropy_model)\n",
    "  decompressor = Imgnet32yDecompressor(entropy_model, trainer.synthesis_transform)\n",
    "  return compressor, decompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "51fd2698-905f-4cb8-bd68-f4f1b1e094ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAC+0lEQVR4nAXBS24cVRQA0Pt7XVVd/bETt9tOHOEoGAnEJAxAiAHLYC2sgwWwCZAygkEGCCSMCANQcExQUNwODna3uz6v3rv3cg5+df/e9Pm6okBEcjBLxbpiD9rs6Mz1xaqWvSLqstVgwCwRlzht6x41NH0XK9zflf82BQcUJ2KW0TyC2So5zmScUscjEwqMJszELNxf1Ym0rFzNtoI15xt5WBeblpAQ0R0gM4c46NbKcVNOYndXwsZKIUREYoTNvHBtm0Y1T2aatmuT0dxbdEKkECQU7Lnfxqq89osls1MnUQGJuOSAgk6biEHS7W2phSqUAoKBrQqs19ghlCWStUOGYIUkUZcklNQlRx8rZSBHEA4KLMToaqTUde3Q9kM2xFFVjkTAHSB//VpN3bOLIYq6na4+WgZTQsyQDf799eOFU1ZTWRmA2g+n26efnpCEwGkCri+rzTmaDtnxSyQHu5lfHm33ZFSVEgT96Y+p/GLhmrITgqoa//n9OeWchy52bXP5M/T+3XXfdXGgfkhZtXjbXJSWNOUhxldPZv3QHT15k9VNzBQxu3vcCG4DApy3925/i5M4/fZYFPBDR/cEOfJSe2YAyO6xSQctMgciAnNHonAIMpgbGDCigyoxu6JTdncF4D2kwVRNjai3DG/NjYGYABSQECZgDg4Z3cwHK8ARFEzlgRnEywXzfO4Ea1kUzqm56NJjVuOJy74j3b46nJ7eWR6Wk5+2j3Hq9voPgvcZG61BnM3YTTczzIpIKdnOwDqNeWCyLsmVjLQNQ8TUV4SOVHN0xhy0B9qa430kREcf+N0zcEcmBoudUQUITigPzYFeLpYvqnd2n1OTpoLmRefzesSqdzZyAu756r2zVK2OPqme/fPBbmG4/gY/GzMqDyomBZ9Vk0eLB/TsuGpofxcMA4HeRdDECT8nt4Q9UJXHPr4ZH2Z2T7/QoxrNdBjkuMx/nVytGIG7eckHNHJzm3u/w4hgWX4vgP9eq4mrYiprQUJo3oDVbJ5z+h8S3+/rqu5j/wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings, entropies = compressor(X_val[0:5])\n",
    "reconstructions = decompressor(strings)\n",
    "display_Y(X_val[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b7e5d1cd-8ae4-439c-a823-03d3ad640220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACMElEQVR4nAXBUbIbRwwDQBDkzOq5fPwcM18pW9LukEC6458kEyvDSSLsCMUwGpHsdq2gaaJE5gQuhdjyHpqoKUaZmSATWER1GjBUpJGqWBE0I8gKb5QFHEGKQSpqY4ncUREVlctSmC2kUgILLyqVRpZfs/vqfZzrhtLMeGohE4kIgPPi8utQe3JohhKFZYVtGGtRjA7WbRSaWEdMBQOT454+Olb8zKoVrJ0VySPMNHtu/Pk+GI+/7JzoOOg9NeqRv8j45K3JPxcO43O7A8S3Syc6ehyH+yd5bY/V+kDij+6u98vxzMc6m3vt3q+FQN+YVxx017GnH338RL2SvN7VrxufS4cV/a33mkd93k84rgRXcf832X95J9On+pmO920/sxzUCvwEnrk6f9XG1EfH3aN5RDvPQRxRDwL//lz7W2/PZ73RPPwC+/2iPgvnk3X0xI/qjkd/feCJXnP7Pj/Tp31GOPtRNR49E+i48QBjNGOOFKfGkfX0tMauY4zjSZm8y92pVGX118/ArQzNA0VTQEw+5don68bpYwnjo6Y8BkyFO57vVfWon2mgGdMHMUBMQLM0m8OaEY4lwG7ZQhByOZlmZrm7Y0COQI9jwsl66lPkQpWDPAGYNAaOSSnFKUQ8uwtMxKStbBHstBnBuX8rRFMhVNW1NlcsBi/nVCz+EnUdF8Vygmxj33vES6+M4Q6XiuXkCEbKmui2hROzFQDTWbCjxJaj6qzq7wXq5a3dNM//RGvov5ApKtEAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_Y(reconstructions[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "864eefa1-c2b8-4989-9902-e6c7824b74c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 21:18:59.131252: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [5000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/40 [============================>.] - ETA: 0s - loss: 218.4374 - distortion_loss: 0.2255 - rate_loss: 105.6871 - distortion_pass_through_loss: 0.2255 - rate_pass_through_loss: 105.6871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 21:19:02.433711: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [100]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 4s 17ms/step - loss: 218.3911 - distortion_loss: 0.2254 - rate_loss: 105.6813 - distortion_pass_through_loss: 0.2242 - rate_pass_through_loss: 105.5957 - val_loss: 204.7802 - val_distortion_loss: 0.2095 - val_rate_loss: 100.0471 - val_distortion_pass_through_loss: 0.2095 - val_rate_pass_through_loss: 100.0471\n",
      "Epoch 2/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 175.1676 - distortion_loss: 0.1476 - rate_loss: 101.3876 - distortion_pass_through_loss: 0.1472 - rate_pass_through_loss: 101.3680 - val_loss: 173.1328 - val_distortion_loss: 0.1473 - val_rate_loss: 99.4652 - val_distortion_pass_through_loss: 0.1473 - val_rate_pass_through_loss: 99.4652\n",
      "Epoch 3/30\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 165.0468 - distortion_loss: 0.1308 - rate_loss: 99.6334 - distortion_pass_through_loss: 0.1305 - rate_pass_through_loss: 99.6134 - val_loss: 168.5641 - val_distortion_loss: 0.1430 - val_rate_loss: 97.0679 - val_distortion_pass_through_loss: 0.1430 - val_rate_pass_through_loss: 97.0679\n",
      "Epoch 4/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 158.5801 - distortion_loss: 0.1221 - rate_loss: 97.5530 - distortion_pass_through_loss: 0.1220 - rate_pass_through_loss: 97.5515 - val_loss: 166.4346 - val_distortion_loss: 0.1432 - val_rate_loss: 94.8297 - val_distortion_pass_through_loss: 0.1432 - val_rate_pass_through_loss: 94.8297\n",
      "Epoch 5/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 154.7969 - distortion_loss: 0.1186 - rate_loss: 95.5077 - distortion_pass_through_loss: 0.1182 - rate_pass_through_loss: 95.4863 - val_loss: 159.6348 - val_distortion_loss: 0.1339 - val_rate_loss: 92.6726 - val_distortion_pass_through_loss: 0.1339 - val_rate_pass_through_loss: 92.6726\n",
      "Epoch 6/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 150.1565 - distortion_loss: 0.1135 - rate_loss: 93.4066 - distortion_pass_through_loss: 0.1132 - rate_pass_through_loss: 93.3898 - val_loss: 149.9693 - val_distortion_loss: 0.1179 - val_rate_loss: 91.0339 - val_distortion_pass_through_loss: 0.1179 - val_rate_pass_through_loss: 91.0339\n",
      "Epoch 7/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 147.4053 - distortion_loss: 0.1121 - rate_loss: 91.3666 - distortion_pass_through_loss: 0.1118 - rate_pass_through_loss: 91.3464 - val_loss: 149.5296 - val_distortion_loss: 0.1216 - val_rate_loss: 88.7217 - val_distortion_pass_through_loss: 0.1216 - val_rate_pass_through_loss: 88.7217\n",
      "Epoch 8/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 144.5729 - distortion_loss: 0.1105 - rate_loss: 89.3326 - distortion_pass_through_loss: 0.1102 - rate_pass_through_loss: 89.3304 - val_loss: 148.5117 - val_distortion_loss: 0.1235 - val_rate_loss: 86.7488 - val_distortion_pass_through_loss: 0.1235 - val_rate_pass_through_loss: 86.7488\n",
      "Epoch 9/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 142.3070 - distortion_loss: 0.1101 - rate_loss: 87.2437 - distortion_pass_through_loss: 0.1100 - rate_pass_through_loss: 87.2033 - val_loss: 141.4503 - val_distortion_loss: 0.1134 - val_rate_loss: 84.7619 - val_distortion_pass_through_loss: 0.1134 - val_rate_pass_through_loss: 84.7619\n",
      "Epoch 10/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 140.0986 - distortion_loss: 0.1097 - rate_loss: 85.2406 - distortion_pass_through_loss: 0.1094 - rate_pass_through_loss: 85.2322 - val_loss: 148.3566 - val_distortion_loss: 0.1331 - val_rate_loss: 81.8040 - val_distortion_pass_through_loss: 0.1331 - val_rate_pass_through_loss: 81.8040\n",
      "Epoch 11/30\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 137.8522 - distortion_loss: 0.1092 - rate_loss: 83.2589 - distortion_pass_through_loss: 0.1089 - rate_pass_through_loss: 83.2237 - val_loss: 137.1389 - val_distortion_loss: 0.1141 - val_rate_loss: 80.0969 - val_distortion_pass_through_loss: 0.1141 - val_rate_pass_through_loss: 80.0969\n",
      "Epoch 12/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 134.9793 - distortion_loss: 0.1073 - rate_loss: 81.3221 - distortion_pass_through_loss: 0.1070 - rate_pass_through_loss: 81.3206 - val_loss: 141.3479 - val_distortion_loss: 0.1277 - val_rate_loss: 77.4971 - val_distortion_pass_through_loss: 0.1277 - val_rate_pass_through_loss: 77.4971\n",
      "Epoch 13/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 134.2168 - distortion_loss: 0.1099 - rate_loss: 79.2569 - distortion_pass_through_loss: 0.1096 - rate_pass_through_loss: 79.2523 - val_loss: 134.6374 - val_distortion_loss: 0.1187 - val_rate_loss: 75.3077 - val_distortion_pass_through_loss: 0.1187 - val_rate_pass_through_loss: 75.3077\n",
      "Epoch 14/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 131.1388 - distortion_loss: 0.1073 - rate_loss: 77.4836 - distortion_pass_through_loss: 0.1071 - rate_pass_through_loss: 77.4322 - val_loss: 130.8584 - val_distortion_loss: 0.1135 - val_rate_loss: 74.1283 - val_distortion_pass_through_loss: 0.1135 - val_rate_pass_through_loss: 74.1283\n",
      "Epoch 15/30\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 129.0221 - distortion_loss: 0.1066 - rate_loss: 75.7083 - distortion_pass_through_loss: 0.1063 - rate_pass_through_loss: 75.7321 - val_loss: 129.6302 - val_distortion_loss: 0.1168 - val_rate_loss: 71.2506 - val_distortion_pass_through_loss: 0.1168 - val_rate_pass_through_loss: 71.2506\n",
      "Epoch 16/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 127.3043 - distortion_loss: 0.1071 - rate_loss: 73.7381 - distortion_pass_through_loss: 0.1068 - rate_pass_through_loss: 73.6960 - val_loss: 125.6921 - val_distortion_loss: 0.1105 - val_rate_loss: 70.4310 - val_distortion_pass_through_loss: 0.1105 - val_rate_pass_through_loss: 70.4310\n",
      "Epoch 17/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 125.4420 - distortion_loss: 0.1068 - rate_loss: 72.0301 - distortion_pass_through_loss: 0.1064 - rate_pass_through_loss: 72.0227 - val_loss: 125.2235 - val_distortion_loss: 0.1152 - val_rate_loss: 67.6254 - val_distortion_pass_through_loss: 0.1152 - val_rate_pass_through_loss: 67.6254\n",
      "Epoch 18/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 123.4192 - distortion_loss: 0.1063 - rate_loss: 70.2890 - distortion_pass_through_loss: 0.1058 - rate_pass_through_loss: 70.3058 - val_loss: 122.8606 - val_distortion_loss: 0.1142 - val_rate_loss: 65.7664 - val_distortion_pass_through_loss: 0.1142 - val_rate_pass_through_loss: 65.7664\n",
      "Epoch 19/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 121.6854 - distortion_loss: 0.1062 - rate_loss: 68.5713 - distortion_pass_through_loss: 0.1058 - rate_pass_through_loss: 68.5623 - val_loss: 122.6279 - val_distortion_loss: 0.1189 - val_rate_loss: 63.1925 - val_distortion_pass_through_loss: 0.1189 - val_rate_pass_through_loss: 63.1925\n",
      "Epoch 20/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 119.9507 - distortion_loss: 0.1061 - rate_loss: 66.9214 - distortion_pass_through_loss: 0.1058 - rate_pass_through_loss: 66.8999 - val_loss: 118.7746 - val_distortion_loss: 0.1139 - val_rate_loss: 61.8490 - val_distortion_pass_through_loss: 0.1139 - val_rate_pass_through_loss: 61.8490\n",
      "Epoch 21/30\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 118.9052 - distortion_loss: 0.1073 - rate_loss: 65.2735 - distortion_pass_through_loss: 0.1068 - rate_pass_through_loss: 65.2775 - val_loss: 116.6614 - val_distortion_loss: 0.1133 - val_rate_loss: 60.0012 - val_distortion_pass_through_loss: 0.1133 - val_rate_pass_through_loss: 60.0012\n",
      "Epoch 22/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 117.2929 - distortion_loss: 0.1071 - rate_loss: 63.7280 - distortion_pass_through_loss: 0.1067 - rate_pass_through_loss: 63.7135 - val_loss: 115.0790 - val_distortion_loss: 0.1126 - val_rate_loss: 58.7944 - val_distortion_pass_through_loss: 0.1126 - val_rate_pass_through_loss: 58.7944\n",
      "Epoch 23/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 116.1186 - distortion_loss: 0.1080 - rate_loss: 62.1198 - distortion_pass_through_loss: 0.1074 - rate_pass_through_loss: 62.1464 - val_loss: 113.6581 - val_distortion_loss: 0.1153 - val_rate_loss: 56.0313 - val_distortion_pass_through_loss: 0.1153 - val_rate_pass_through_loss: 56.0313\n",
      "Epoch 24/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 114.0799 - distortion_loss: 0.1067 - rate_loss: 60.7060 - distortion_pass_through_loss: 0.1063 - rate_pass_through_loss: 60.7188 - val_loss: 113.0781 - val_distortion_loss: 0.1182 - val_rate_loss: 53.9599 - val_distortion_pass_through_loss: 0.1182 - val_rate_pass_through_loss: 53.9599\n",
      "Epoch 25/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 112.8180 - distortion_loss: 0.1071 - rate_loss: 59.2595 - distortion_pass_through_loss: 0.1066 - rate_pass_through_loss: 59.2761 - val_loss: 110.1130 - val_distortion_loss: 0.1148 - val_rate_loss: 52.7291 - val_distortion_pass_through_loss: 0.1148 - val_rate_pass_through_loss: 52.7291\n",
      "Epoch 26/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 111.3309 - distortion_loss: 0.1069 - rate_loss: 57.8966 - distortion_pass_through_loss: 0.1064 - rate_pass_through_loss: 57.9065 - val_loss: 109.6137 - val_distortion_loss: 0.1176 - val_rate_loss: 50.8189 - val_distortion_pass_through_loss: 0.1176 - val_rate_pass_through_loss: 50.8189\n",
      "Epoch 27/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 110.2755 - distortion_loss: 0.1074 - rate_loss: 56.5550 - distortion_pass_through_loss: 0.1070 - rate_pass_through_loss: 56.5746 - val_loss: 108.6180 - val_distortion_loss: 0.1201 - val_rate_loss: 48.5875 - val_distortion_pass_through_loss: 0.1201 - val_rate_pass_through_loss: 48.5875\n",
      "Epoch 28/30\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 108.8086 - distortion_loss: 0.1072 - rate_loss: 55.2196 - distortion_pass_through_loss: 0.1067 - rate_pass_through_loss: 55.2211 - val_loss: 109.4044 - val_distortion_loss: 0.1257 - val_rate_loss: 46.5456 - val_distortion_pass_through_loss: 0.1257 - val_rate_pass_through_loss: 46.5456\n",
      "Epoch 29/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 108.1024 - distortion_loss: 0.1083 - rate_loss: 53.9756 - distortion_pass_through_loss: 0.1078 - rate_pass_through_loss: 53.9659 - val_loss: 107.6187 - val_distortion_loss: 0.1250 - val_rate_loss: 45.1351 - val_distortion_pass_through_loss: 0.1250 - val_rate_pass_through_loss: 45.1351\n",
      "Epoch 30/30\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 106.8364 - distortion_loss: 0.1083 - rate_loss: 52.6821 - distortion_pass_through_loss: 0.1079 - rate_pass_through_loss: 52.7058 - val_loss: 106.2802 - val_distortion_loss: 0.1249 - val_rate_loss: 43.8416 - val_distortion_pass_through_loss: 0.1249 - val_rate_pass_through_loss: 43.8416\n"
     ]
    }
   ],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((X,y))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_val,y_val))\n",
    "trainer = train_imgnet32y_model(lmbda=500)\n",
    "compressor, decompressor = make_imgnet32y_codec(trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
