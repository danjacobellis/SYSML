{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IqR2PQG4ZaZ0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 23:10:55.687141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 23:10:56.432251: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 23:10:56.432307: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 23:10:56.432315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from balle import *\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_decision_forests as tfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "465/469 [============================>.] - ETA: 0s - loss: 159.3805 - distortion_loss: 0.0637 - rate_loss: 95.6470 - distortion_pass_through_loss: 0.0637 - rate_pass_through_loss: 95.6470"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Computing quantization offsets using offset heuristic within a tf.function. Ideally, the offset heuristic should only be used to determine offsets once after training. Depending on the prior, estimating the offset might be computationally expensive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 8s 11ms/step - loss: 159.1602 - distortion_loss: 0.0636 - rate_loss: 95.5657 - distortion_pass_through_loss: 0.0636 - rate_pass_through_loss: 95.5602 - val_loss: 133.1790 - val_distortion_loss: 0.0477 - val_rate_loss: 85.4781 - val_distortion_pass_through_loss: 0.0477 - val_rate_pass_through_loss: 85.4804\n",
      "Epoch 2/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 122.5249 - distortion_loss: 0.0468 - rate_loss: 75.7591 - distortion_pass_through_loss: 0.0468 - rate_pass_through_loss: 75.7541 - val_loss: 112.2437 - val_distortion_loss: 0.0471 - val_rate_loss: 65.1413 - val_distortion_pass_through_loss: 0.0471 - val_rate_pass_through_loss: 65.1484\n",
      "Epoch 3/15\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 107.4754 - distortion_loss: 0.0471 - rate_loss: 60.3287 - distortion_pass_through_loss: 0.0471 - rate_pass_through_loss: 60.3256 - val_loss: 99.4346 - val_distortion_loss: 0.0505 - val_rate_loss: 48.9248 - val_distortion_pass_through_loss: 0.0505 - val_rate_pass_through_loss: 48.9445\n",
      "Epoch 4/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 98.7381 - distortion_loss: 0.0483 - rate_loss: 50.4243 - distortion_pass_through_loss: 0.0483 - rate_pass_through_loss: 50.4213 - val_loss: 91.6884 - val_distortion_loss: 0.0537 - val_rate_loss: 37.9818 - val_distortion_pass_through_loss: 0.0537 - val_rate_pass_through_loss: 38.0110\n",
      "Epoch 5/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 93.6545 - distortion_loss: 0.0492 - rate_loss: 44.4062 - distortion_pass_through_loss: 0.0492 - rate_pass_through_loss: 44.4045 - val_loss: 87.1984 - val_distortion_loss: 0.0565 - val_rate_loss: 30.7034 - val_distortion_pass_through_loss: 0.0565 - val_rate_pass_through_loss: 30.7160\n",
      "Epoch 6/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 90.3342 - distortion_loss: 0.0498 - rate_loss: 40.5709 - distortion_pass_through_loss: 0.0498 - rate_pass_through_loss: 40.5699 - val_loss: 83.0596 - val_distortion_loss: 0.0565 - val_rate_loss: 26.5649 - val_distortion_pass_through_loss: 0.0565 - val_rate_pass_through_loss: 26.5587\n",
      "Epoch 7/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 87.8324 - distortion_loss: 0.0499 - rate_loss: 37.8867 - distortion_pass_through_loss: 0.0499 - rate_pass_through_loss: 37.8852 - val_loss: 80.5431 - val_distortion_loss: 0.0564 - val_rate_loss: 24.0940 - val_distortion_pass_through_loss: 0.0565 - val_rate_pass_through_loss: 24.0927\n",
      "Epoch 8/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 85.6028 - distortion_loss: 0.0497 - rate_loss: 35.8555 - distortion_pass_through_loss: 0.0497 - rate_pass_through_loss: 35.8546 - val_loss: 78.5509 - val_distortion_loss: 0.0532 - val_rate_loss: 25.3492 - val_distortion_pass_through_loss: 0.0532 - val_rate_pass_through_loss: 25.3339\n",
      "Epoch 9/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 83.4808 - distortion_loss: 0.0493 - rate_loss: 34.2039 - distortion_pass_through_loss: 0.0493 - rate_pass_through_loss: 34.2032 - val_loss: 76.2343 - val_distortion_loss: 0.0519 - val_rate_loss: 24.3272 - val_distortion_pass_through_loss: 0.0519 - val_rate_pass_through_loss: 24.3203\n",
      "Epoch 10/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 81.4992 - distortion_loss: 0.0487 - rate_loss: 32.7692 - distortion_pass_through_loss: 0.0487 - rate_pass_through_loss: 32.7684 - val_loss: 75.0009 - val_distortion_loss: 0.0502 - val_rate_loss: 24.8129 - val_distortion_pass_through_loss: 0.0502 - val_rate_pass_through_loss: 24.8081\n",
      "Epoch 11/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 79.7189 - distortion_loss: 0.0480 - rate_loss: 31.6746 - distortion_pass_through_loss: 0.0480 - rate_pass_through_loss: 31.6739 - val_loss: 74.2167 - val_distortion_loss: 0.0493 - val_rate_loss: 24.8920 - val_distortion_pass_through_loss: 0.0493 - val_rate_pass_through_loss: 24.8797\n",
      "Epoch 12/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 78.3215 - distortion_loss: 0.0475 - rate_loss: 30.7759 - distortion_pass_through_loss: 0.0475 - rate_pass_through_loss: 30.7748 - val_loss: 74.0138 - val_distortion_loss: 0.0494 - val_rate_loss: 24.6293 - val_distortion_pass_through_loss: 0.0494 - val_rate_pass_through_loss: 24.6379\n",
      "Epoch 13/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 77.0763 - distortion_loss: 0.0470 - rate_loss: 30.0630 - distortion_pass_through_loss: 0.0470 - rate_pass_through_loss: 30.0626 - val_loss: 72.7395 - val_distortion_loss: 0.0476 - val_rate_loss: 25.1827 - val_distortion_pass_through_loss: 0.0475 - val_rate_pass_through_loss: 25.1774\n",
      "Epoch 14/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 76.1174 - distortion_loss: 0.0467 - rate_loss: 29.4421 - distortion_pass_through_loss: 0.0467 - rate_pass_through_loss: 29.4415 - val_loss: 72.5588 - val_distortion_loss: 0.0469 - val_rate_loss: 25.6590 - val_distortion_pass_through_loss: 0.0469 - val_rate_pass_through_loss: 25.6617\n",
      "Epoch 15/15\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 75.1760 - distortion_loss: 0.0462 - rate_loss: 28.9519 - distortion_pass_through_loss: 0.0462 - rate_pass_through_loss: 28.9514 - val_loss: 71.8014 - val_distortion_loss: 0.0460 - val_rate_loss: 25.7868 - val_distortion_pass_through_loss: 0.0460 - val_rate_pass_through_loss: 25.7893\n"
     ]
    }
   ],
   "source": [
    "trainer = train_mnist_model(lmbda=1000)\n",
    "compressor, decompressor = make_mnist_codec(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent(x,y):\n",
    "    x = tf.cast(tf.expand_dims(x,axis=0),tf.float32) / 255.\n",
    "    z = tf.round(compressor.analysis_transform(x)[0])\n",
    "    z = tf.cast(z, tf.int8)\n",
    "    return z,y\n",
    "\n",
    "training_dataset2 = training_dataset.map(get_latent).batch(100)\n",
    "validation_dataset2 = validation_dataset.map(get_latent).batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 12 thread(s) for training\n",
      "Use /tmp/tmp89eou137 as temporary training directory\n",
      "Reading training dataset...\n",
      "Training tensor examples:\n",
      "Features: Tensor(\"data:0\", shape=(None, 50), dtype=int8)\n",
      "Label: Tensor(\"data_1:0\", shape=(None,), dtype=int64)\n",
      "Weights: None\n",
      "Normalized tensor features:\n",
      " {'data:0.0': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice:0' shape=(None,) dtype=float32>), 'data:0.1': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_1:0' shape=(None,) dtype=float32>), 'data:0.2': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_2:0' shape=(None,) dtype=float32>), 'data:0.3': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_3:0' shape=(None,) dtype=float32>), 'data:0.4': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_4:0' shape=(None,) dtype=float32>), 'data:0.5': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_5:0' shape=(None,) dtype=float32>), 'data:0.6': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_6:0' shape=(None,) dtype=float32>), 'data:0.7': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_7:0' shape=(None,) dtype=float32>), 'data:0.8': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_8:0' shape=(None,) dtype=float32>), 'data:0.9': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_9:0' shape=(None,) dtype=float32>), 'data:0.10': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_10:0' shape=(None,) dtype=float32>), 'data:0.11': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_11:0' shape=(None,) dtype=float32>), 'data:0.12': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_12:0' shape=(None,) dtype=float32>), 'data:0.13': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_13:0' shape=(None,) dtype=float32>), 'data:0.14': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_14:0' shape=(None,) dtype=float32>), 'data:0.15': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_15:0' shape=(None,) dtype=float32>), 'data:0.16': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_16:0' shape=(None,) dtype=float32>), 'data:0.17': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_17:0' shape=(None,) dtype=float32>), 'data:0.18': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_18:0' shape=(None,) dtype=float32>), 'data:0.19': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_19:0' shape=(None,) dtype=float32>), 'data:0.20': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_20:0' shape=(None,) dtype=float32>), 'data:0.21': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_21:0' shape=(None,) dtype=float32>), 'data:0.22': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_22:0' shape=(None,) dtype=float32>), 'data:0.23': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_23:0' shape=(None,) dtype=float32>), 'data:0.24': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_24:0' shape=(None,) dtype=float32>), 'data:0.25': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_25:0' shape=(None,) dtype=float32>), 'data:0.26': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_26:0' shape=(None,) dtype=float32>), 'data:0.27': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_27:0' shape=(None,) dtype=float32>), 'data:0.28': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_28:0' shape=(None,) dtype=float32>), 'data:0.29': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_29:0' shape=(None,) dtype=float32>), 'data:0.30': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_30:0' shape=(None,) dtype=float32>), 'data:0.31': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_31:0' shape=(None,) dtype=float32>), 'data:0.32': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_32:0' shape=(None,) dtype=float32>), 'data:0.33': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_33:0' shape=(None,) dtype=float32>), 'data:0.34': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_34:0' shape=(None,) dtype=float32>), 'data:0.35': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_35:0' shape=(None,) dtype=float32>), 'data:0.36': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_36:0' shape=(None,) dtype=float32>), 'data:0.37': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_37:0' shape=(None,) dtype=float32>), 'data:0.38': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_38:0' shape=(None,) dtype=float32>), 'data:0.39': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_39:0' shape=(None,) dtype=float32>), 'data:0.40': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_40:0' shape=(None,) dtype=float32>), 'data:0.41': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_41:0' shape=(None,) dtype=float32>), 'data:0.42': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_42:0' shape=(None,) dtype=float32>), 'data:0.43': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_43:0' shape=(None,) dtype=float32>), 'data:0.44': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_44:0' shape=(None,) dtype=float32>), 'data:0.45': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_45:0' shape=(None,) dtype=float32>), 'data:0.46': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_46:0' shape=(None,) dtype=float32>), 'data:0.47': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_47:0' shape=(None,) dtype=float32>), 'data:0.48': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_48:0' shape=(None,) dtype=float32>), 'data:0.49': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_49:0' shape=(None,) dtype=float32>)}\n",
      "Training dataset read in 0:01:49.881543. Found 60000 examples.\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2023-03-21T23:25:17.148445058-05:00 kernel.cc:756] Start Yggdrasil model training\n",
      "[INFO 2023-03-21T23:25:17.148467299-05:00 kernel.cc:757] Collect training examples\n",
      "[INFO 2023-03-21T23:25:17.148683407-05:00 kernel.cc:388] Number of batches: 600\n",
      "[INFO 2023-03-21T23:25:17.148694298-05:00 kernel.cc:389] Number of examples: 60000\n",
      "[INFO 2023-03-21T23:25:17.175253498-05:00 kernel.cc:774] Training dataset:\n",
      "Number of records: 60000\n",
      "Number of columns: 51\n",
      "\n",
      "Number of columns by type:\n",
      "\tNUMERICAL: 50 (98.0392%)\n",
      "\tCATEGORICAL: 1 (1.96078%)\n",
      "\n",
      "Columns:\n",
      "\n",
      "NUMERICAL: 50 (98.0392%)\n",
      "\t1: \"data:0.0\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t2: \"data:0.1\" NUMERICAL mean:0.0358833 min:-5 max:4 sd:1.25276\n",
      "\t3: \"data:0.10\" NUMERICAL mean:-0.0805 min:-5 max:3 sd:1.16285\n",
      "\t4: \"data:0.11\" NUMERICAL mean:-0.0216667 min:-2 max:2 sd:0.623081\n",
      "\t5: \"data:0.12\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t6: \"data:0.13\" NUMERICAL mean:-0.0170167 min:-1 max:2 sd:0.342287\n",
      "\t7: \"data:0.14\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t8: \"data:0.15\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t9: \"data:0.16\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t10: \"data:0.17\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t11: \"data:0.18\" NUMERICAL mean:-0.0117333 min:-2 max:2 sd:0.375094\n",
      "\t12: \"data:0.19\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t13: \"data:0.2\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t14: \"data:0.20\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t15: \"data:0.21\" NUMERICAL mean:-0.032 min:-4 max:5 sd:1.31839\n",
      "\t16: \"data:0.22\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t17: \"data:0.23\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t18: \"data:0.24\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t19: \"data:0.25\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t20: \"data:0.26\" NUMERICAL mean:-0.0241667 min:-2 max:2 sd:0.597424\n",
      "\t21: \"data:0.27\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t22: \"data:0.28\" NUMERICAL mean:0.03305 min:-3 max:3 sd:0.845157\n",
      "\t23: \"data:0.29\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t24: \"data:0.3\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t25: \"data:0.30\" NUMERICAL mean:-0.01745 min:-3 max:4 sd:1.02379\n",
      "\t26: \"data:0.31\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t27: \"data:0.32\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t28: \"data:0.33\" NUMERICAL mean:-0.0123 min:-2 max:2 sd:0.525625\n",
      "\t29: \"data:0.34\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t30: \"data:0.35\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t31: \"data:0.36\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t32: \"data:0.37\" NUMERICAL mean:-0.0126667 min:-3 max:3 sd:0.769788\n",
      "\t33: \"data:0.38\" NUMERICAL mean:0.0063 min:-1 max:2 sd:0.337135\n",
      "\t34: \"data:0.39\" NUMERICAL mean:-0.0120333 min:-2 max:3 sd:0.661328\n",
      "\t35: \"data:0.4\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t36: \"data:0.40\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t37: \"data:0.41\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t38: \"data:0.42\" NUMERICAL mean:-0.0243333 min:-3 max:4 sd:1.08153\n",
      "\t39: \"data:0.43\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t40: \"data:0.44\" NUMERICAL mean:-0.0001 min:-1 max:1 sd:0.0611009\n",
      "\t41: \"data:0.45\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t42: \"data:0.46\" NUMERICAL mean:0.0114 min:-4 max:4 sd:1.22435\n",
      "\t43: \"data:0.47\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t44: \"data:0.48\" NUMERICAL mean:-0.0100833 min:-2 max:3 sd:0.601732\n",
      "\t45: \"data:0.49\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t46: \"data:0.5\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t47: \"data:0.6\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t48: \"data:0.7\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t49: \"data:0.8\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\t50: \"data:0.9\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
      "\n",
      "CATEGORICAL: 1 (1.96078%)\n",
      "\t0: \"__LABEL\" CATEGORICAL integerized vocab-size:11 no-ood-item\n",
      "\n",
      "Terminology:\n",
      "\tnas: Number of non-available (i.e. missing) values.\n",
      "\tood: Out of dictionary.\n",
      "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
      "\ttokenized: The attribute value is obtained through tokenization.\n",
      "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
      "\tvocab-size: Number of unique values.\n",
      "\n",
      "[INFO 2023-03-21T23:25:17.175338899-05:00 kernel.cc:790] Configure learner\n",
      "[INFO 2023-03-21T23:25:17.175594852-05:00 kernel.cc:804] Training config:\n",
      "learner: \"RANDOM_FOREST\"\n",
      "features: \"^data:0\\\\.0$\"\n",
      "features: \"^data:0\\\\.1$\"\n",
      "features: \"^data:0\\\\.10$\"\n",
      "features: \"^data:0\\\\.11$\"\n",
      "features: \"^data:0\\\\.12$\"\n",
      "features: \"^data:0\\\\.13$\"\n",
      "features: \"^data:0\\\\.14$\"\n",
      "features: \"^data:0\\\\.15$\"\n",
      "features: \"^data:0\\\\.16$\"\n",
      "features: \"^data:0\\\\.17$\"\n",
      "features: \"^data:0\\\\.18$\"\n",
      "features: \"^data:0\\\\.19$\"\n",
      "features: \"^data:0\\\\.2$\"\n",
      "features: \"^data:0\\\\.20$\"\n",
      "features: \"^data:0\\\\.21$\"\n",
      "features: \"^data:0\\\\.22$\"\n",
      "features: \"^data:0\\\\.23$\"\n",
      "features: \"^data:0\\\\.24$\"\n",
      "features: \"^data:0\\\\.25$\"\n",
      "features: \"^data:0\\\\.26$\"\n",
      "features: \"^data:0\\\\.27$\"\n",
      "features: \"^data:0\\\\.28$\"\n",
      "features: \"^data:0\\\\.29$\"\n",
      "features: \"^data:0\\\\.3$\"\n",
      "features: \"^data:0\\\\.30$\"\n",
      "features: \"^data:0\\\\.31$\"\n",
      "features: \"^data:0\\\\.32$\"\n",
      "features: \"^data:0\\\\.33$\"\n",
      "features: \"^data:0\\\\.34$\"\n",
      "features: \"^data:0\\\\.35$\"\n",
      "features: \"^data:0\\\\.36$\"\n",
      "features: \"^data:0\\\\.37$\"\n",
      "features: \"^data:0\\\\.38$\"\n",
      "features: \"^data:0\\\\.39$\"\n",
      "features: \"^data:0\\\\.4$\"\n",
      "features: \"^data:0\\\\.40$\"\n",
      "features: \"^data:0\\\\.41$\"\n",
      "features: \"^data:0\\\\.42$\"\n",
      "features: \"^data:0\\\\.43$\"\n",
      "features: \"^data:0\\\\.44$\"\n",
      "features: \"^data:0\\\\.45$\"\n",
      "features: \"^data:0\\\\.46$\"\n",
      "features: \"^data:0\\\\.47$\"\n",
      "features: \"^data:0\\\\.48$\"\n",
      "features: \"^data:0\\\\.49$\"\n",
      "features: \"^data:0\\\\.5$\"\n",
      "features: \"^data:0\\\\.6$\"\n",
      "features: \"^data:0\\\\.7$\"\n",
      "features: \"^data:0\\\\.8$\"\n",
      "features: \"^data:0\\\\.9$\"\n",
      "label: \"^__LABEL$\"\n",
      "task: CLASSIFICATION\n",
      "random_seed: 123456\n",
      "metadata {\n",
      "  framework: \"TF Keras\"\n",
      "}\n",
      "pure_serving_model: false\n",
      "[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n",
      "  num_trees: 300\n",
      "  decision_tree {\n",
      "    max_depth: 16\n",
      "    min_examples: 5\n",
      "    in_split_min_examples_check: true\n",
      "    keep_non_leaf_label_distribution: true\n",
      "    num_candidate_attributes: 0\n",
      "    missing_value_policy: GLOBAL_IMPUTATION\n",
      "    allow_na_conditions: false\n",
      "    categorical_set_greedy_forward {\n",
      "      sampling: 0.1\n",
      "      max_num_items: -1\n",
      "      min_item_frequency: 1\n",
      "    }\n",
      "    growing_strategy_local {\n",
      "    }\n",
      "    categorical {\n",
      "      cart {\n",
      "      }\n",
      "    }\n",
      "    axis_aligned_split {\n",
      "    }\n",
      "    internal {\n",
      "      sorting_strategy: PRESORTED\n",
      "    }\n",
      "    uplift {\n",
      "      min_examples_in_treatment: 5\n",
      "      split_score: KULLBACK_LEIBLER\n",
      "    }\n",
      "  }\n",
      "  winner_take_all_inference: true\n",
      "  compute_oob_performances: true\n",
      "  compute_oob_variable_importances: false\n",
      "  num_oob_variable_importances_permutations: 1\n",
      "  bootstrap_training_dataset: true\n",
      "  bootstrap_size_ratio: 1\n",
      "  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n",
      "  sampling_with_replacement: true\n",
      "}\n",
      "\n",
      "[INFO 2023-03-21T23:25:17.175703747-05:00 kernel.cc:807] Deployment config:\n",
      "cache_path: \"/tmp/tmp89eou137/working_cache\"\n",
      "num_threads: 12\n",
      "try_resume_training: true\n",
      "\n",
      "[INFO 2023-03-21T23:25:17.175875231-05:00 kernel.cc:868] Train model\n",
      "[INFO 2023-03-21T23:25:17.177269932-05:00 random_forest.cc:415] Training random forest on 60000 example(s) and 50 feature(s).\n",
      "[INFO 2023-03-21T23:25:17.749560124-05:00 random_forest.cc:804] Training of tree  1/300 (tree index:4) done accuracy:0.821928 logloss:6.41838\n",
      "[INFO 2023-03-21T23:25:17.903305058-05:00 random_forest.cc:804] Training of tree  11/300 (tree index:10) done accuracy:0.854325 logloss:2.48192\n",
      "[INFO 2023-03-21T23:25:18.41075804-05:00 random_forest.cc:804] Training of tree  21/300 (tree index:20) done accuracy:0.872287 logloss:1.53143\n",
      "[INFO 2023-03-21T23:25:18.974252947-05:00 random_forest.cc:804] Training of tree  31/300 (tree index:30) done accuracy:0.879633 logloss:1.11426\n",
      "[INFO 2023-03-21T23:25:19.493451059-05:00 random_forest.cc:804] Training of tree  41/300 (tree index:40) done accuracy:0.88255 logloss:0.942802\n",
      "[INFO 2023-03-21T23:25:20.017181911-05:00 random_forest.cc:804] Training of tree  51/300 (tree index:50) done accuracy:0.884767 logloss:0.832959\n",
      "[INFO 2023-03-21T23:25:20.507154116-05:00 random_forest.cc:804] Training of tree  61/300 (tree index:60) done accuracy:0.885817 logloss:0.765905\n",
      "[INFO 2023-03-21T23:25:20.688338067-05:00 random_forest.cc:804] Training of tree  71/300 (tree index:69) done accuracy:0.88665 logloss:0.702905\n",
      "[INFO 2023-03-21T23:25:21.25512008-05:00 random_forest.cc:804] Training of tree  81/300 (tree index:80) done accuracy:0.886933 logloss:0.668516\n",
      "[INFO 2023-03-21T23:25:21.770436872-05:00 random_forest.cc:804] Training of tree  91/300 (tree index:90) done accuracy:0.888133 logloss:0.636988\n",
      "[INFO 2023-03-21T23:25:22.276863315-05:00 random_forest.cc:804] Training of tree  101/300 (tree index:99) done accuracy:0.88885 logloss:0.613516\n",
      "[INFO 2023-03-21T23:25:22.794784097-05:00 random_forest.cc:804] Training of tree  111/300 (tree index:110) done accuracy:0.88905 logloss:0.594189\n",
      "[INFO 2023-03-21T23:25:23.250497781-05:00 random_forest.cc:804] Training of tree  121/300 (tree index:120) done accuracy:0.889167 logloss:0.580314\n",
      "[INFO 2023-03-21T23:25:23.520761499-05:00 random_forest.cc:804] Training of tree  131/300 (tree index:130) done accuracy:0.890133 logloss:0.562443\n",
      "[INFO 2023-03-21T23:25:24.041168502-05:00 random_forest.cc:804] Training of tree  141/300 (tree index:140) done accuracy:0.890417 logloss:0.550004\n",
      "[INFO 2023-03-21T23:25:24.56128441-05:00 random_forest.cc:804] Training of tree  151/300 (tree index:149) done accuracy:0.890617 logloss:0.539084\n",
      "[INFO 2023-03-21T23:25:25.056481157-05:00 random_forest.cc:804] Training of tree  161/300 (tree index:160) done accuracy:0.890783 logloss:0.527306\n",
      "[INFO 2023-03-21T23:25:25.530616186-05:00 random_forest.cc:804] Training of tree  171/300 (tree index:170) done accuracy:0.890983 logloss:0.518201\n",
      "[INFO 2023-03-21T23:25:26.034375627-05:00 random_forest.cc:804] Training of tree  181/300 (tree index:180) done accuracy:0.891267 logloss:0.508171\n",
      "[INFO 2023-03-21T23:25:26.319434533-05:00 random_forest.cc:804] Training of tree  191/300 (tree index:190) done accuracy:0.891117 logloss:0.501521\n",
      "[INFO 2023-03-21T23:25:26.849759778-05:00 random_forest.cc:804] Training of tree  201/300 (tree index:199) done accuracy:0.891167 logloss:0.4956\n",
      "[INFO 2023-03-21T23:25:27.363836698-05:00 random_forest.cc:804] Training of tree  211/300 (tree index:210) done accuracy:0.891217 logloss:0.4896\n",
      "[INFO 2023-03-21T23:25:27.825548791-05:00 random_forest.cc:804] Training of tree  221/300 (tree index:220) done accuracy:0.8912 logloss:0.480026\n",
      "[INFO 2023-03-21T23:25:28.310636257-05:00 random_forest.cc:804] Training of tree  231/300 (tree index:229) done accuracy:0.891283 logloss:0.475493\n",
      "[INFO 2023-03-21T23:25:28.787815542-05:00 random_forest.cc:804] Training of tree  241/300 (tree index:240) done accuracy:0.891867 logloss:0.472207\n",
      "[INFO 2023-03-21T23:25:29.140861309-05:00 random_forest.cc:804] Training of tree  251/300 (tree index:250) done accuracy:0.891783 logloss:0.46464\n",
      "[INFO 2023-03-21T23:25:29.657565272-05:00 random_forest.cc:804] Training of tree  261/300 (tree index:260) done accuracy:0.8918 logloss:0.461415\n",
      "[INFO 2023-03-21T23:25:30.149040124-05:00 random_forest.cc:804] Training of tree  271/300 (tree index:270) done accuracy:0.8918 logloss:0.458538\n",
      "[INFO 2023-03-21T23:25:30.632413462-05:00 random_forest.cc:804] Training of tree  281/300 (tree index:280) done accuracy:0.891933 logloss:0.456855\n",
      "[INFO 2023-03-21T23:25:31.122269194-05:00 random_forest.cc:804] Training of tree  291/300 (tree index:292) done accuracy:0.8923 logloss:0.453516\n",
      "[INFO 2023-03-21T23:25:31.326384315-05:00 random_forest.cc:804] Training of tree  300/300 (tree index:299) done accuracy:0.8919 logloss:0.451337\n",
      "[INFO 2023-03-21T23:25:31.326554677-05:00 random_forest.cc:884] Final OOB metrics: accuracy:0.8919 logloss:0.451337\n",
      "[INFO 2023-03-21T23:25:32.023003772-05:00 kernel.cc:905] Export model in log directory: /tmp/tmp89eou137 with prefix cb027539b0864783\n",
      "[INFO 2023-03-21T23:25:33.378551291-05:00 kernel.cc:923] Save model in resources\n",
      "[INFO 2023-03-21T23:25:33.382077143-05:00 abstract_model.cc:849] Model self evaluation:\n",
      "Number of predictions (without weights): 60000\n",
      "Number of predictions (with weights): 60000\n",
      "Task: CLASSIFICATION\n",
      "Label: __LABEL\n",
      "\n",
      "Accuracy: 0.8919  CI95[W][0.889793 0.893979]\n",
      "LogLoss: : 0.451337\n",
      "ErrorRate: : 0.1081\n",
      "\n",
      "Default Accuracy: : 0.112367\n",
      "Default LogLoss: : 2.30116\n",
      "Default ErrorRate: : 0.887633\n",
      "\n",
      "Confusion Table:\n",
      "truth\\prediction\n",
      "    0     1     2     3     4     5     6     7     8     9    10\n",
      " 0  0     0     0     0     0     0     0     0     0     0     0\n",
      " 1  0  5615     3    51    20     9    77    94    10    28    16\n",
      " 2  0     0  6553    46    30    20    18    17    31    18     9\n",
      " 3  0    83    49  5435    97    46    25    41    61   107    14\n",
      " 4  0    43    48   225  5208    13   220    21    85   236    32\n",
      " 5  0    20    26    67    10  5149    27    65    79    33   366\n",
      " 6  0    87    33    59   292    38  4542    97    37   155    81\n",
      " 7  0    90    37    80    12    43    73  5542     4    34     3\n",
      " 8  0    20    43    78    24    86    43     4  5688    18   261\n",
      " 9  0    51    58   155   212    62   240    50    39  4857   127\n",
      "10  0    48    25    32    59   362    85     6   327    80  4925\n",
      "Total: 60000\n",
      "\n",
      "One vs other classes:\n",
      "\n",
      "[INFO 2023-03-21T23:25:33.888148492-05:00 kernel.cc:1214] Loading model from path /tmp/tmp89eou137/model/ with prefix cb027539b0864783\n",
      "[INFO 2023-03-21T23:25:37.551847037-05:00 decision_forest.cc:661] Model loaded with 300 root(s), 1746982 node(s), and 17 input feature(s).\n",
      "[INFO 2023-03-21T23:25:37.551878005-05:00 abstract_model.cc:1311] Engine \"RandomForestGeneric\" built\n",
      "[INFO 2023-03-21T23:25:37.551908553-05:00 kernel.cc:1046] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 0:00:20.698140\n",
      "Compiling model...\n",
      "Model compiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5efefbd8d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tfdf.keras.RandomForestModel(verbose=2)\n",
    "model.fit(training_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 20s 194ms/step - loss: 0.0000e+00 - accuracy: 0.8913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.8913000226020813]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(metrics=[\"accuracy\"])\n",
    "model.evaluate(validation_dataset2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "data_compression.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
