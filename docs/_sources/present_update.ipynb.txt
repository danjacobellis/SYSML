{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e4f8c-85dd-4f36-9ee4-0462952afe5d",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "# Transfer Learning from Lossy Codecs\n",
    "[Slides](https://danjacobellis.github.io/SYSML/present_update.slides.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e072a5-d669-4465-9a53-03cf7f807acd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<script>\n",
    "    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';\n",
    "</script>\n",
    "\n",
    "<center> <h1>\n",
    "Transfer Learning from Lossy Codecs\n",
    "</h1> </center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center> <h2>\n",
    "Dan Jacobellis\n",
    "</h2> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa1078-7e0d-44c5-8f0d-99f43c450981",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lossy compression\n",
    "\n",
    "* Most data are stored using lossy formats (MP3, JPEG)\n",
    "* 1-4 bit subband quantization is typical\n",
    "* ~1.5 bits per sample/pixel after entropy coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b6296-15c4-49c9-a70b-01362df4bdfd",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/lossy_lossless.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415e03e-48e4-41da-b49d-89bc78f7f7f6",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/lossy_lossless.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012104d-ab48-462e-a8ca-2bc3788cc92a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conventional training procedure\n",
    "\n",
    "* Still suffers from all of the downsides of lossy compression\n",
    "* Don't get any of the benefits of smaller representation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42577d58-eb38-4c27-9c6c-61568a1ad349",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/conventional.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f030fd9-c281-48e6-96ef-61cce8442f94",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/conventional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907b2a7-49d9-4ec7-9f15-31a059ca1259",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The neural codecs are coming!\n",
    "\n",
    "* Google: Soundstream/Lyra (2021) \n",
    "  * [API available for web applications and android](https://github.com/google/lyra)\n",
    "  * Currently used in Google meet for low bitrate connections\n",
    "* Meta: Encodec (2022)\n",
    "  * [Pytorch API available](https://github.com/facebookresearch/encodec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6e009-0556-40ab-b0e7-10935824d179",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/encodec_architecture.png\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f7b0a-0d2d-4d28-9c2f-e27a73ee1c54",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/encodec_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feaa4de-ce6c-4814-9669-4fae7cecdee0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Neural image/video compression\n",
    "\n",
    "* Many patents have been filed. Expect standardized versions very soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a425e8-2490-47f5-ad84-4e29e58b111c",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/JPEG_vs_SD.svg\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5bd7c8-e91d-4db0-95de-78752da53a8a",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/JPEG_vs_SD.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0213ee-2d91-4ace-bb45-d7a036037823",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Neural representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41dcaf-aa1c-41e0-9ae4-36007f6b98e1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/vae.svg\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc9f8e-adfa-43eb-90ab-79945709ef3e",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/vae.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02baec2-62f3-48ee-a69f-551fe9a65193",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Scaling convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52884f5f-d9d3-449f-9bcf-5c93cf9c5dcf",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/EfficientNet.svg\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6cf24-2294-4734-9ca0-1007d18578ae",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/EfficientNet.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d6cb8-5acc-4000-a2ef-abc9510a203f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Scaling convolutional neural networks\n",
    "\n",
    "* Depth: $d=\\alpha^{\\phi}$\n",
    "* Width: $w=\\beta^{\\phi}$\n",
    "* Resolution: $d=\\gamma^{\\phi}$\n",
    "* $\\alpha=1.2, \\beta=1.1, \\gamma=1.15$\n",
    "* FLOPS $\\propto \\alpha \\beta^2 \\gamma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0171ee4-7d72-44fa-9efe-76c6acf36e4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Initial results\n",
    " \n",
    "|    Model    |        Input Size       | Accuracy | Parameters | Training Time | Training FLOPS |\n",
    "|:-----------:|:-----------------------:|:--------:|:----------:|:-------------:|:--------------:|\n",
    "| MobileNetV2 | $$224\\times224\\times3$$ |    58%   |    2.23M   |  32 sec/epoch |      6.1 T     |\n",
    "|   Resample  |  $$64\\times64\\times3$$  |    39%   |    250K    |  14 sec/epoch |     0.915 B    |\n",
    "|     VAE     |  $$64\\times64\\times4$$  |    44%   |    251K    |  15 sec/epoch |     0.976 B    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291d55e-55a4-48ca-bc6c-d815620023da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Linear decoding of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2b04f-5a64-4df6-803d-5e77edb40551",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/linear_decode1.svg\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0bd44d-7881-402c-adf4-f9af7e9517fc",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/linear_decode1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a90d0-ca52-4565-9a87-52a2d127ce8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Linear decoding of latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1949a362-c277-408a-9f55-4697a9a8c0b0",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<img src=\"_images/linear_decode2.svg\" width=700 height=700 class=\"center\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18796820-c741-44f4-8bf8-f49b5bfb77ab",
   "metadata": {
    "tags": [
     "remove-nb-cell"
    ]
   },
   "source": [
    "![](img/linear_decode2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6b8ec-3987-4514-a677-a266ee3548d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Next steps\n",
    " \n",
    "* Search for better architecture for initial layers\n",
    "* Larger batch sizes can fit in memory $\\to$ need larger dataset\n",
    "* Explore efficient pipelines for augmentation\n",
    "* Test effects of quantization with similar model\n",
    "* Test other types of models more suited to discrete inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0-beta3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
