
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Approaches to neural compression &#8212; Dan Jacobellis | University of Texas at Austin</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Transfer Learning from Lossy Codecs" href="proposal.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="approaches-to-neural-compression">
<h1>Approaches to neural compression<a class="headerlink" href="#approaches-to-neural-compression" title="Permalink to this headline">¶</a></h1>
<section id="optimize-parameters-of-a-nonlinear-transform-code">
<h2>Optimize parameters of a nonlinear transform code<a class="headerlink" href="#optimize-parameters-of-a-nonlinear-transform-code" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="http://www.cns.nyu.edu/pub/eero/balle17a-final.pdf">Paper: End-to-end optimized image compression</a></p>
<ul>
<li><p><a class="reference external" href="https://www.tensorflow.org/tutorials/generative/data_compression">Code example (tensorflow documentation)</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Neural_Data-Dependent_Transform_for_Learned_Image_Compression_CVPR_2022_paper.html">Paper: Neural Data-Dependent Transform for Learned Image Compression</a></p>
<ul>
<li><p><a class="reference external" href="https://github.com/Dezhao-Wang/Neural-Syntax-Code">Code example with pretrained model</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="vector-quantized-variational-autoencoder">
<h2>Vector-quantized variational autoencoder<a class="headerlink" href="#vector-quantized-variational-autoencoder" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html">Paper: Neural Discrete Representation Learning</a></p>
<ul>
<li><p><a class="reference external" href="https://keras.io/examples/generative/vq_vae/">Code example (keras documentation)</a></p></li>
<li><p><a class="reference external" href="https://github.com/facebookresearch/encodec">Standardized codec for speech and music: “Encodec”</a></p></li>
<li><p><a class="reference external" href="https://danjacobellis.net/ITML/discrete_representation_learning.html">Dan’s slides on VQ-VAE</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="rnn-based-generative-model-of-speech">
<h2>RNN-based generative model of speech<a class="headerlink" href="#rnn-based-generative-model-of-speech" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9415120?casa_token=dZRQjc-xqesAAAAA:UxxPxExec7YEAFOdHvM5L0fPMa3LjVNz8UJpeqoAQEwUds6j5ng5Nik5SnPcBlGsPQT2q2HG">Paper: Generative speech coding with predictive variance regularization</a></p>
<ul>
<li><p><a class="reference external" href="https://github.com/google/lyra">Standardized codec for speech only: “Lyra”</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="conditional-gan-for-images">
<h2>Conditional GAN for images<a class="headerlink" href="#conditional-gan-for-images" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/8a50bae297807da9e97722a0b3fd8f27-Abstract.html">Paper: High-Fidelity Generative Image Compression</a></p>
<ul>
<li><p><a class="reference external" href="https://github.com/tensorflow/compression/tree/master/models/hific">Code example with pretrained model</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="neural-network-structures-for-learning-from-quantized-data">
<h1>Neural network structures for learning from quantized data<a class="headerlink" href="#neural-network-structures-for-learning-from-quantized-data" title="Permalink to this headline">¶</a></h1>
<section id="binary-neural-networks">
<h2>Binary Neural Networks<a class="headerlink" href="#binary-neural-networks" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.larq.dev/larq/">Larq: Library for implementing BNNs</a></p></li>
<li><p><a class="reference external" href="https://danjacobellis.net/ITML/lossy_learning.slides.html#/">Dan’s slides</a></p></li>
</ul>
</section>
<section id="one-hot-encode-then-exploit-sparsity">
<h2>One-hot encode, then exploit sparsity<a class="headerlink" href="#one-hot-encode-then-exploit-sparsity" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.nature.com/articles/s41598-023-27986-6">Paper: Learning on tree architectures outperforms a convolutional feedforward network</a></p>
<ul>
<li><p><a class="reference external" href="https://github.com/yuval-meir/Tree-3">Code example for CIFAR</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="transfer-learning-self-supervised-learning">
<h1>Transfer learning / Self supervised learning<a class="headerlink" href="#transfer-learning-self-supervised-learning" title="Permalink to this headline">¶</a></h1>
<section id="conventional-transfer-learning-from-pretrained-mobilenet">
<h2>Conventional transfer learning from pretrained mobilenet<a class="headerlink" href="#conventional-transfer-learning-from-pretrained-mobilenet" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/tutorials/images/transfer_learning">Moblilenet v2 (images)</a></p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/tutorials/audio/transfer_learning_audio">YAMNet (audio)</a></p></li>
</ul>
</section>
<section id="self-supervised-learning">
<h2>Self supervised learning<a class="headerlink" href="#self-supervised-learning" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.isca-speech.org/archive_v0/Interspeech_2019/abstracts/1873.html">wav2vec</a></p>
<ul>
<li><p><a class="reference external" href="https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/README.md">wav2vec 2.0 on github</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="datasets-and-models-for-experiments">
<h1>Datasets and models for experiments<a class="headerlink" href="#datasets-and-models-for-experiments" title="Permalink to this headline">¶</a></h1>
<section id="images">
<h2>Images<a class="headerlink" href="#images" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Imagenet-1k and Mobilenet variants</p>
<ul>
<li><p>Mobilenet models allow tradeoff between model complexity and accuracy</p></li>
<li><p>Two main hyperparameters: Width multiplier and resolution multiplier</p>
<ul>
<li><p>Width multipler <span class="math notranslate nohighlight">\(\alpha \in (0,1]\)</span> controls the number of channels at each layer. Computational cost is proportional to <span class="math notranslate nohighlight">\(\alpha^2\)</span></p></li>
<li><p>Resolution multiplier <span class="math notranslate nohighlight">\(\rho \in (0,1]\)</span> controls the resolution of each channel. Computational cost is proportional to <span class="math notranslate nohighlight">\(\rho^2\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img alt="" src="_images/top1_vs_MAdd.png" /></p>
<p><img alt="" src="_images/top1_vs_latency.png" /></p>
<p><a class="reference external" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.html">source</a></p>
</section>
<section id="audio">
<h2>Audio<a class="headerlink" href="#audio" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Current audio models use similar CNN to images, but applied to a time-frequency representation of the audio</p></li>
<li><p><a class="reference external" href="https://tfhub.dev/google/yamnet/1">Pretrained model: YAMNet</a></p>
<ul>
<li><p><a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/audioset/yamnet">github repo describing pipeline</a></p>
<ul>
<li><p>Data is resampled to 16 kHz</p></li>
<li><p>A time-frequency transform is applied</p></li>
<li><p>input size is 96x64</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://research.google.com/audioset/">Audioset</a></p>
<ul>
<li><p><a class="reference external" href="https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv">Subset consisting of 521 classes</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">SysML</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="reading_list.html">Reading list</a></li>
<li class="toctree-l1"><a class="reference internal" href="proposal.html">Transfer Learning from Lossy Codecs</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Approaches to neural compression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#optimize-parameters-of-a-nonlinear-transform-code">Optimize parameters of a nonlinear transform code</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vector-quantized-variational-autoencoder">Vector-quantized variational autoencoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rnn-based-generative-model-of-speech">RNN-based generative model of speech</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conditional-gan-for-images">Conditional GAN for images</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#neural-network-structures-for-learning-from-quantized-data">Neural network structures for learning from quantized data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#binary-neural-networks">Binary Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#one-hot-encode-then-exploit-sparsity">One-hot encode, then exploit sparsity</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#transfer-learning-self-supervised-learning">Transfer learning / Self supervised learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#conventional-transfer-learning-from-pretrained-mobilenet">Conventional transfer learning from pretrained mobilenet</a></li>
<li class="toctree-l2"><a class="reference internal" href="#self-supervised-learning">Self supervised learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#datasets-and-models-for-experiments">Datasets and models for experiments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="#audio">Audio</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="proposal.html" title="previous chapter">Transfer Learning from Lossy Codecs</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/progress.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>