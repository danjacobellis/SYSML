{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e4f8c-85dd-4f36-9ee4-0462952afe5d",
   "metadata": {
    "tags": [
     "remove-nb-cell",
     "remove-cell"
    ]
   },
   "source": [
    "# Transfer Learning from Lossy Codecs\n",
    "[Slides](https://danjacobellis.github.io/SYSML/progress.slides.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e072a5-d669-4465-9a53-03cf7f807acd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<script>\n",
    "    document.querySelector('head').innerHTML += '<style>.slides { zoom: 1.75 !important; }</style>';\n",
    "</script>\n",
    "\n",
    "<center> <h1>\n",
    "Transfer Learning from Lossy Codecs\n",
    "</h1> </center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center> <h2>\n",
    "Dan Jacobellis\n",
    "</h2> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4adb1d4-ea2b-486f-87e3-b4163663fdb2",
   "metadata": {},
   "source": [
    "## Approaches to neural compression\n",
    "\n",
    "### Transformer\n",
    "\n",
    "* [Paper: Variable-Rate Deep Image Compression With Vision Transformers](https://ieeexplore.ieee.org/abstract/document/9770776)\n",
    "\n",
    "### Optimize parameters of a nonlinear transform code\n",
    "* [Paper: End-to-end optimized image compression](http://www.cns.nyu.edu/pub/eero/balle17a-final.pdf)\n",
    "  * [Code example (tensorflow documentation)](https://www.tensorflow.org/tutorials/generative/data_compression)\n",
    "* [Paper: Neural Data-Dependent Transform for Learned Image Compression](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Neural_Data-Dependent_Transform_for_Learned_Image_Compression_CVPR_2022_paper.html)\n",
    "  * [Code example with pretrained model](https://github.com/Dezhao-Wang/Neural-Syntax-Code)\n",
    "\n",
    "### Vector-quantized variational autoencoder\n",
    "* [Paper: Neural Discrete Representation Learning](https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html)\n",
    "  * [Code example (keras documentation)](https://keras.io/examples/generative/vq_vae/)\n",
    "  * [Standardized codec for speech and music: \"Encodec\"](https://github.com/facebookresearch/encodec)\n",
    "  * [Dan's slides on VQ-VAE](https://danjacobellis.net/ITML/discrete_representation_learning.html)\n",
    "\n",
    "### RNN-based generative model of speech\n",
    "* [Paper: Generative speech coding with predictive variance regularization](https://ieeexplore.ieee.org/abstract/document/9415120?casa_token=dZRQjc-xqesAAAAA:UxxPxExec7YEAFOdHvM5L0fPMa3LjVNz8UJpeqoAQEwUds6j5ng5Nik5SnPcBlGsPQT2q2HG)\n",
    "  * [Standardized codec for speech only: \"Lyra\"](https://github.com/google/lyra)\n",
    "\n",
    "### Conditional GAN for images\n",
    "\n",
    "* [Paper: High-Fidelity Generative Image Compression](https://proceedings.neurips.cc/paper/2020/hash/8a50bae297807da9e97722a0b3fd8f27-Abstract.html)\n",
    "  * [Code example with pretrained model](https://github.com/tensorflow/compression/tree/master/models/hific)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3adb3d1-90c8-410f-b294-fd6a3444bcc7",
   "metadata": {},
   "source": [
    "## Neural network structures for learning from quantized data\n",
    "\n",
    "### Binary Neural Networks\n",
    "\n",
    "* [Larq: Library for implementing BNNs](https://docs.larq.dev/larq/)\n",
    "* [Dan's slides](https://danjacobellis.net/ITML/lossy_learning.slides.html#/)\n",
    "\n",
    "### One-hot encode, then exploit sparsity\n",
    "* [Paper: Learning on tree architectures outperforms a convolutional feedforward network](https://www.nature.com/articles/s41598-023-27986-6)\n",
    "  * [Code example for CIFAR](https://github.com/yuval-meir/Tree-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda47045-c7ed-4924-9319-707ea68423c1",
   "metadata": {},
   "source": [
    "## Transfer learning / Self supervised learning\n",
    "\n",
    "### Conventional transfer learning from pretrained mobilenet\n",
    "\n",
    "* [Moblilenet v2 (images)](https://www.tensorflow.org/tutorials/images/transfer_learning)\n",
    "\n",
    "* [YAMNet (audio)](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio)\n",
    "\n",
    "### Self supervised learning\n",
    "\n",
    "* [wav2vec](https://www.isca-speech.org/archive_v0/Interspeech_2019/abstracts/1873.html)\n",
    "  * [wav2vec 2.0 on github](https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/README.md)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a684bc-aaa9-4234-8e15-f89826171ad0",
   "metadata": {},
   "source": [
    "## Datasets and models for experiments\n",
    "\n",
    "### Images\n",
    "\n",
    "* Imagenet-1k and Mobilenet variants\n",
    "  * Mobilenet models allow tradeoff between model complexity and accuracy\n",
    "  * Two main hyperparameters: Width multiplier and resolution multiplier\n",
    "    * Width multipler $\\alpha \\in (0,1]$ controls the number of channels at each layer. Computational cost is proportional to $\\alpha^2$\n",
    "    * Resolution multiplier $\\rho \\in (0,1]$ controls the resolution of each channel. Computational cost is proportional to $\\rho^2$\n",
    "\n",
    "![](img/top1_vs_MAdd.png)\n",
    "\n",
    "![](img/top1_vs_latency.png)\n",
    "\n",
    "[source](https://openaccess.thecvf.com/content_ICCV_2019/html/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.html)\n",
    "\n",
    "### Audio\n",
    "\n",
    "* Current audio models use similar CNN to images, but applied to a time-frequency representation of the audio\n",
    "\n",
    "* [Pretrained model: YAMNet](https://tfhub.dev/google/yamnet/1)\n",
    "  * [github repo describing pipeline](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet)\n",
    "    * Data is resampled to 16 kHz\n",
    "    * A time-frequency transform is applied\n",
    "    * input size is 96x64\n",
    "  * [Audioset](https://research.google.com/audioset/)\n",
    "    * [Subset consisting of 521 classes](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0-beta3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
